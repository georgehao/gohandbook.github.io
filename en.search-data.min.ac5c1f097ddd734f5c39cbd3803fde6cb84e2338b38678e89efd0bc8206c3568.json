[{"id":0,"href":"/gohandbook.github.io/docs/rand-chapter/2021-01-23-rand/","title":"Math/rand","section":"随机数","content":"一文完全掌握 Go math/rand #  Go 获取随机数是开发中经常会用到的功能, 不过这个里面还是有一些坑存在的, 本文将完全剖析 Go math/rand, 让你轻松使用 Go Rand.\n开篇一问: 你觉得 rand 会 panic 吗 ?\n 源码剖析 #  math/rand 源码其实很简单, 就两个比较重要的函数\nfunc (rng *rngSource) Seed(seed int64) { rng.tap = 0 rng.feed = rngLen - rngTap //... \tx := int32(seed) for i := -20; i \u0026lt; rngLen; i++ { x = seedrand(x) if i \u0026gt;= 0 { var u int64 u = int64(x) \u0026lt;\u0026lt; 40 x = seedrand(x) u ^= int64(x) \u0026lt;\u0026lt; 20 x = seedrand(x) u ^= int64(x) u ^= rngCooked[i] rng.vec[i] = u } } } 这个函数就是在设置 seed, 其实就是对 rng.vec 各个位置设置对应的值. rng.vec 的大小是 607.\nfunc (rng *rngSource) Uint64() uint64 { rng.tap-- if rng.tap \u0026lt; 0 { rng.tap += rngLen } rng.feed-- if rng.feed \u0026lt; 0 { rng.feed += rngLen } x := rng.vec[rng.feed] + rng.vec[rng.tap] rng.vec[rng.feed] = x return uint64(x) } 我们在使用不管调用 Intn(), Int31n() 等其他函数, 最终调用到就是这个函数. 可以看到每次调用就是利用 rng.feed rng.tap 从 rng.vec 中取到两个值相加的结果返回了. 同时还是这个结果又重新放入 rng.vec.\n在这里需要注意使用 rng.go 的 rngSource 时, 由于 rng.vec 在获取随机数时会同时设置 rng.vec 的值, 当多 goroutine 同时调用时就会有数据竞争问题. math/rand 采用在调用 rngSource 时加锁 sync.Mutex 解决.\nfunc (r *lockedSource) Uint64() (n uint64) { r.lk.Lock() n = r.src.Uint64() r.lk.Unlock() return } 另外我们能直接使用 rand.Seed(), rand.Intn(100), 是因为 math/rand 初始化了一个全局的 globalRand 变量.\nvar globalRand = New(\u0026amp;lockedSource{src: NewSource(1).(*rngSource)}) func Seed(seed int64) { globalRand.Seed(seed) } func Uint32() uint32 { return globalRand.Uint32() } 需要注意到由于调用 rngSource 加了锁, 所以直接使用 rand.Int32() 会导致全局的 goroutine 锁竞争, 所以在高并发场景时, 当你的程序的性能是卡在这里的话, 你需要考虑利用 New(\u0026amp;lockedSource{src: NewSource(1).(*rngSource)}) 为不同的模块生成单独的 rand. 不过根据目前的实践来看, 使用全局的 globalRand 锁竞争并没有我们想象中那么激烈. 使用 New 生成新的 rand 里面是有坑的, 开篇的 panic 就是这么产生的, 后面具体再说.\n种子(seed)到底起什么作用 ? #  func main() { for i := 0; i \u0026lt; 10; i++ { fmt.Printf(\u0026#34;current:%d\\n\u0026#34;, time.Now().Unix()) rand.Seed(time.Now().Unix()) fmt.Println(rand.Intn(100)) } } 结果:\ncurrent:1613814632 65 current:1613814632 65 current:1613814632 65 ... 这个例子能得出一个结论: 相同种子，每次运行的结果都是一样的. 这是为什么呢?\n在使用 math/rand 的时候, 一定需要通过调用 rand.Seed 来设置种子, 其实就是给 rng.vec 的 607 个槽设置对应的值. 通过上面的源码那可以看出来, rand.Seed 会调用一个 seedrand 的函数, 来计算对应槽的值.\nfunc seedrand(x int32) int32 { const ( A = 48271 Q = 44488 R = 3399 ) hi := x / Q lo := x % Q x = A*lo - R*hi if x \u0026lt; 0 { x += int32max } return x } 这个函数的计算结果并不是随机的, 而是根据 seed 实际算出来的. 另外这个函数并不是随便写的, 是有相关的数学证明的.\n这也导致了相同的 seed, 最终设置到 rng.vec里面的值是相同的, 通过 Intn 取出的也是相同的值\n我遇到的那些坑 #  1. rand panic #  文章开头的截图就是项目开发中使用别人封装的底层库, 在某天出现的 panic. 大概实现的代码\n// random.go  var ( rrRand = rand.New(rand.NewSource(time.Now().Unix())) ) type Random struct{} func (r *Random) Balance(sf *service.Service) ([]string, error) { // .. 通过服务发现获取到一堆ip+port, 然后随机拿到其中的一些ip和port出来 \trandIndexes := rrRand.Perm(randMax) // 返回这些ip 和port } 这个 Random 会被并发调用, 由于 rrRand 不是并发安全的, 所以就导致了调用 rrRand.Perm 时偶尔会出现 panic 情况.\n在使用 math/rand 的时候, 有些人使用 math.Intn() 看了下注释发现是全局共享了一个锁, 担心出现锁竞争, 所以用 rand.New 来初始化一个新的 rand, 但是要注意到 rand.New 初始化出来的 rand 并不是并发安全的.\n修复方案: 就是把 rrRand 换成了 globalRand, 在线上高并发场景下, 发现全局锁影响并不大.\n2. 获取的都是同一个机器 #   同样也是底层封装的 rpc 库, 使用 random 的方式来流量分发, 在线上跑了一段时间后, 流量都路由到一台机器上了, 导致服务直接宕机. 大概实现代码:\nfunc Call(ctx *gin.Context, method string, service string, data map[string]interface{}) (buf []byte, err error) { ins, err := ral.GetInstance(ctx, ral.TYPE_HTTP, service) if err != nil { // 错误处理 \t} defer ins.Release() if b, e := ins.Request(ctx, method, data, head); e == nil { // 错误处理 \t} // 其他逻辑, 重试等等 } func GetInstance(ctx *gin.Context, modType string, name string) (*Instance, error) { // 其他逻辑..  switch res.Strategy { case WITH_RANDOM: if res.rand == nil { res.rand = rand.New(rand.NewSource(time.Now().Unix())) } which = res.rand.Intn(res.count) case 其他负载均衡查了 } // 返回其中一个ip和port } 引起问题的原因: 可以看出来每次请求到来都是利用 GetInstance 来获取一个 ip 和 port, 如果采用 Random 方式的流量负载均衡, 每次都是重新初始化一个 rand. 我们已经知道当设置相同的种子，每次运行的结果都是一样的. 当瞬间流量过大时, 并发请求 GetInstance, 由于那一刻 time.Now().Unix() 的值是一样的, 这样就会导致获取到随机数都是一样的, 所以就导致最后获取到的 ip, port都是一样的, 流量都分发到这台机器上了.\n修复方案: 修改成 globalRand 即可.\nrand 未来期望 #  说到这里基本上可以看出来, 为了防止全局锁竞争问题, 在使用 math/rand 的时候, 首先都会想到自定义 rand, 但是就容易整出来莫名其妙的问题.\n为什么 math/rand 需要加锁呢?\n大家都知道 math/rand 是伪随机的, 但是在设置完 seed 后, rng.vec 数组的值基本上就确定下来了, 这明显就不是随机了, 为了增加随机性, 通过 Uint64() 获取到随机数后, 还会重新去设置 rng.vec. 由于存在并发获取随机数的需求, 也就有了并发设置 rng.vec 的值, 所以需要对 rng.vec 加锁保护.\n使用 rand.Intn() 确实会有全局锁竞争问题, 你觉得 math/rand 未来会优化吗? 以及如何优化? 欢迎留言讨论\n更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":1,"href":"/gohandbook.github.io/docs/sync-chapter/2021-04-01-mutex/","title":"Mutex","section":"并发原语","content":"这可能是最容易理解的 Go Mutex 源码剖析 #  Hi，大家好，我是 haohongfan。\n上一篇文章《一文完全掌握 Go math/rand》，我们知道 math/rand 的 global rand 有一个全局锁，我的文章里面有一句话：“修复方案: 就是把 rrRand 换成了 globalRand, 在线上高并发场景下, 发现全局锁影响并不大.”， 有同学私聊我“他们遇到线上服务的锁竞争特别激烈”。确实我这句话说的并不严谨。但是也让我有了一个思考：到底多高的 QPS 才能让 Mutex 产生强烈的锁竞争 ？\n到底加锁的代码会不会产生线上问题？ 到底该不该使用锁来实现这个功能？线上的问题是不是由于使用了锁造成的？针对这些问题，本文就从源码角度剖析 Go Mutex, 揭开 Mutex 的迷雾。\n源码分析 #  Go mutex 源码只有短短的 228 行，但是却包含了很多的状态转变在里面，很不容易看懂，具体可以参见下面的流程图。Mutex 的实现主要借助了 CAS 指令 + 自旋 + 信号量来实现，具体代码我就不再每一行做分析了，有兴趣的可以根据下面流程图配合源码阅读一番。\nLock #   Unlock #   一些例子 #   一个 goroutine 加锁解锁过程   没有加锁，直接解锁问题    两个 Goroutine，互相加锁解锁   三个 Goroutine 等待加锁过程   整篇源码其实涉及比较难以理解的就是 Mutex 状态（mutexLocked，mutexWoken，mutexStarving，mutexWaiterShift） 与 Goroutine 之间的状态（starving，awoke）改变， 我们下面将逐一说明。\n什么是 Goroutine 排队? #   如果 Mutex 已经被一个 Goroutine 获取了锁, 其它等待中的 Goroutine 们只能一直等待。那么等这个锁释放后，等待中的 Goroutine 中哪一个会优先获取 Mutex 呢?\n正常情况下, 当一个 Goroutine 获取到锁后, 其他的 Goroutine 开始进入自旋转(为了持有CPU) 或者进入沉睡阻塞状态(等待信号量唤醒). 但是这里存在一个问题, 新请求的 Goroutine 进入自旋时是仍然拥有 CPU 的, 所以比等待信号量唤醒的 Goroutine 更容易获取锁. 用官方话说就是，新请求锁的 Goroutine具有优势，它正在CPU上执行，而且可能有好几个，所以刚刚唤醒的 Goroutine 有很大可能在锁竞争中失败.\n于是如果一个 Goroutine 被唤醒过后, 仍然没有拿到锁, 那么该 Goroutine 会放在等待队列的最前面. 并且那些等待超过 1 ms 的 Goroutine 还没有获取到锁，该 Goroutine 就会进入饥饿状态。该 Goroutine 是饥饿状态并且 Mutex 是 Locked 状态时，才有可能给 Mutex 设置成饥饿状态.\n获取到锁的 Goroutine Unlock, 将 Mutex 的 Locked 状态解除, 发出来解锁信号, 等待的 Goroutine 开始竞争该信号. 如果发现当前 Mutex 是饥饿状态, 直接将唤醒信号发给第一个等待的 Goroutine\n这就是所谓的 Goroutine 排队\n排队功能是如何实现的 #  我们知道在正常状态下，所有等待锁的 Goroutine 按照 FIFO 顺序等待，在 Mutex 饥饿状态下，会直接把释放锁信号发给等待队列中的第一个Goroutine。排队功能主要是通过 runtime_SemacquireMutex, runtime_Semrelease 来实现的.\n一、runtime_SemacquireMutex \u0026ndash; 入队\n当 Mutex 被其他 Goroutine 持有时，新来的 Goroutine 将会被 runtime_SemacquireMutex 阻塞。阻塞会分为2种情况:\nGoroutine 第一次被阻塞：\n当 Goroutine 第一次尝试获取锁时，由于当前锁可能不能被锁定，于是有可能进入下面逻辑\nqueueLifo := waitStartTime != 0 if waitStartTime == 0 { waitStartTime = runtime_nanotime() } runtime_SemacquireMutex(\u0026amp;m.sema, queueLifo, 1) 由于 waitStartTime 等于 0，runtime_SemacquireMutex 的 queueLifo 等于 false, 于是该 Goroutine 放入到队列的尾部。\nGoroutine 被唤醒过，但是没加锁成功，再次被阻塞\n由于 Goroutine 被唤醒过，waitStartTime 不等于 0，runtime_SemacquireMutex 的 queueLifo 等于 true, 于是该 Goroutine 放入到队列的头部。\n二、runtime_Semrelease \u0026ndash; 出队\n当某个 Goroutine 释放锁时，调用 Unlock，这里同样存在两种情况：\n当前 mutex 不是饥饿状态\nif new\u0026amp;mutexStarving == 0 { old := new for { if old\u0026gt;\u0026gt;mutexWaiterShift == 0 || old\u0026amp;(mutexLocked|mutexWoken|mutexStarving) != 0 { return } // Grab the right to wake someone.  new = (old - 1\u0026lt;\u0026lt;mutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(\u0026amp;m.state, old, new) { runtime_Semrelease(\u0026amp;m.sema, false, 1) return } old = m.state } } Unlock 时 Mutex 的 Locked 状态被去掉。当发现当前 Mutex 不是饥饿状态，设置 runtime_Semrelease 的 handoff 参数是 false, 于是唤醒其中一个 Goroutine。\n当前 mutex 已经是饥饿状态\n} else { // Starving mode: handoff mutex ownership to the next waiter, and yield  // our time slice so that the next waiter can start to run immediately.  // Note: mutexLocked is not set, the waiter will set it after wakeup.  // But mutex is still considered locked if mutexStarving is set,  // so new coming goroutines won\u0026#39;t acquire it.  runtime_Semrelease(\u0026amp;m.sema, true, 1) } 同样 Unlock 时 Mutex 的 Locked 状态被去掉。由于当前 Mutex 是饥饿状态，于是设置 runtime_Semrelease 的 handoff 参数是 true, 于是让等待队列头部的第一个 Goroutine 获得锁。\nGoroutine 的排队 与 mutex 中记录的 Waiters 之间的关系? #  通过上面的分析，我们知道 Goroutine 的排队是通过 runtime_SemacquireMutex 来实现的。Mutex.state 记录了目前通过 runtime_SemacquireMutex 排队的 Goroutine 的数量\nGoroutine 的饥饿与 Mutex 饥饿之间的关系？ #  Goroutine 的状态跟 Mutex 的是息息相关的。只有在 Goroutine 是饥饿状态下，才有可能给 Mutex 设置成饥饿状态。在 Mutex 是饥饿状态时，才有可能让饥饿的 Goroutine 优先获取到锁。不过需要注意的是，触发 Mutex 饥饿的 Goroutine 并不一定获取锁，有可能被其他的饥饿的 Goroutine 截胡。\nGoroutine 能够加锁成功的情况 #  Mutex 没有被 Goroutine 占用 Mutex.state = 0, 这种情况下一定能获取到锁. 例如: 第一个 Goroutine 获取到锁 还有一种情况 Goroutine有可能加锁成功:\n 当前 Mutex 不是饥饿状态, 也不是 Locked 状态, 尝试 CAS 加锁时, Mutex 的值还没有被其他 Goroutine 改变, 当前 Goroutine 才能加锁成功. 某个 Goroutine 刚好被唤醒后, 重新获取 Mutex, 这个时候 Mutex 处于饥饿状态. 因为这个时候只唤醒了饥饿的 Goroutine, 其他的 Goroutine 都在排队中, 没有其他 Goroutine 来竞争 Mutex, 所以能直接加锁成功  Mutex 锁竞争的相关问题 #  探测锁竞争 #  日常开发中锁竞争的问题还是能经常遇到的，我们如何去发现锁竞争呢？其实还是需要靠 pprof 来人肉来分析。\n《一次错误使用 go-cache 导致出现的线上问题》就是我真是遇到的一次线上问题，表象就是接口大量超时，打开pprof 发现大量 Goroutine 都集中 Lock 上。这个真实场景的具体的分析过程，有兴趣的可以阅读一下。 简单总结一下： 压测或者流量高的时候发现系统不正常，打开 pprof 发现 goroutine 指标在飙升，并且大量 Goroutine 都阻塞在 Mutex 的 Lock 上，这个基本就可以确定是锁竞争。\npprof 里面是有个 pprof/mutex 指标，不过该指标默认是关闭的，而且并没有太多资料有介绍这个指标如何来分析 Mutex。有知道这个指标怎么用的大佬，欢迎留言。\nmutex 锁的瓶颈 #  现在模拟业务开发中的某接口，平均耗时 10 ms, 在 32C 物理机上压测。CentOS Linux release 7.3.1611 (Core), go1.15.8 压测代码如下：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; ) var mux sync.Mutex func testMutex(w http.ResponseWriter, r *http.Request) { mux.Lock() time.Sleep(10 * time.Millisecond) mux.Unlock() } func main() { go func() { log.Println(http.ListenAndServe(\u0026#34;:6060\u0026#34;, nil)) }() http.HandleFunc(\u0026#34;/test/mutex\u0026#34;, testMutex) if err := http.ListenAndServe(\u0026#34;:8000\u0026#34;, nil); err != nil { fmt.Println(\u0026#34;start http server fail:\u0026#34;, err) } }   这个例子写的比较极端了，全局共享一个 Mutex。经过压测发现在 100 qps 时，Mutex 没啥竞争，在 150 QPS 时竞争就开始变的激烈了。\n当然我们写业务代码并不会这么写，但是可以通过这个例子发现 Mutex 在 QPS 很低的时候，锁竞争就会很激烈。需要说明的一点：这个压测数值没啥具体的意义，不同的机器上表现肯定还会不一样。\n这个例子告诉我们几点：\n 写业务时不能全局使用同一个 Mutex 尽量避免使用 Mutex，如果非使用不可，尽量多声明一些 Mutex，采用取模分片的方式去使用其中一个 Mutex  日常使用注意点 #  1. Lock/Unlock 成对出现 #  我们日常开发中使用 Mutex 一定要记得：先 Lock 再 Unlock。\n特别要注意的是：没有 Lock 就去 Unlock。当然这个 case 一般情况下我们都不会这么写。不过有些变种的写法我们要尤其注意，例如\nvar mu sync.Mutex func release() { mu.Lock() fmt.Println(\u0026#34;lock1 success\u0026#34;) time.Sleep(10 * time.Second) mu.Lock() fmt.Println(\u0026#34;lock2 success\u0026#34;) } func main() { go release() time.Sleep(time.Second) mu.Unlock() fmt.Println(\u0026#34;unlock success\u0026#34;) for {} } 输出结果：\nrelease lock1 success main unlock success release lock2 success 我们看到 release goroutine 的锁竟然被 main goroutine 给释放了，同时 release goroutine 又能重新获取到锁。\n这段代码可能你想不到有啥问题，其实这个问题蛮严重的，想象一下你的代码中，本来是要加锁给用户加积分的，但是竟然被别的 goroutine 给解锁了，导致积分没有增加成功，同时解锁的时候还别的 Goroutine 的锁给 Unlock 了，互相加锁解锁，导致莫名其妙的问题。\n所以一般情况下，要在本 Goroutine 中完成 Mutex 的 Lock\u0026amp;Unlock，千万不要将要加锁和解锁分到两个 Goroutine 中进行。如果你确实需要这么做，请抽支烟冷静一下，你真的是否需要这么做。\n2. Mutex 千万不能被复制 #  我之前发过的《当 Go struct 遇上 Mutex》里面详细分析了不能被复制的原因，以及如何 Mutex 的最佳使用方式，建议没看过的同学去看一遍。我们还是举个例子说下为啥不能被复制，以及如何用源码进行分析\ntype Person struct { mux sync.Mutex } func Reduce(p1 Person) { fmt.Println(\u0026#34;step...\u0026#34;, ) p1.mux.Lock() fmt.Println(p1) defer p1.mux.Unlock() fmt.Println(\u0026#34;over...\u0026#34;) } func main() { var p Person p.mux.Lock() go Reduce(p) p.mux.Unlock() fmt.Println(111) for {} } 问题分析：\n main Goroutine 已经给 p.mux 加了锁 , 这个时候 p.mux 的 state 的值是 mutexLocked。 然后将 p.mux 复制给了 Reduce Goroutine。这个时候被复制的 p1.mux 的 state 的值也是 mutexLocked。 main Goroutine 虽然已经解锁了, 但是 Reduce Goroutine 跟 main Goroutine 的 mutex 已经不是同一个 mutex 了, 所以 Reduce Goroutine 就会加锁失败, 产生死锁，关键是编译器还发现不了这个 Deadlock.  关于为什么编译器不能发现这个死锁，可以看我的博客《一次 Golang Deadlock 的讨论》\n至此 Go Mutex 的源码剖析全部完毕了，有什么想跟我交流的可以再评论区留言。\n更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":2,"href":"/gohandbook.github.io/docs/timer-chapter/2021-04-01-timer/","title":"Timer","section":"定时器","content":"Go timer 是如何被调度的？ #  hi，大家好，我是 haohongfan。\n本篇文章剖析下 Go 定时器的相关内容。定时器不管是业务开发，还是基础架构开发，都是绕不过去的存在，由此可见定时器的重要程度。\n我们不管用 NewTimer, timer.After，还是 timer.AfterFun 来初始化一个 timer, 这个 timer 最终都会加入到一个全局 timer 堆中，由 Go runtime 统一管理。\n全局的 timer 堆也经历过三个阶段的重要升级。\n Go 1.9 版本之前，所有的计时器由全局唯一的四叉堆维护，协程间竞争激烈。 Go 1.10 - 1.13，全局使用 64 个四叉堆维护全部的计时器，没有本质解决 1.9 版本之前的问题 Go 1.14 版本之后，每个 P 单独维护一个四叉堆。  Go 1.14 以后的 timer 性能得到了质的飞升，不过伴随而来的是 timer 成了 Go 里面最复杂、最难梳理的数据结构。本文不会详细分析每一个细节，我们从大体来了解 Go timer 的工作原理。\n1. 使用场景 #  Go timer 在我们代码中会经常遇到。\n场景1：RPC 调用的防超时处理（下面代码节选 dubbogo)\nfunc (c *Client) Request(request *remoting.Request, timeout time.Duration, response *remoting.PendingResponse) error { _, session, err := c.selectSession(c.addr) // .. 省略  if totalLen, sendLen, err = c.transfer(session, request, timeout); err != nil { if sendLen != 0 \u0026amp;\u0026amp; totalLen != sendLen { // .. 省略  } return perrors.WithStack(err) } // .. 省略  select { case \u0026lt;-getty.GetTimeWheel().After(timeout): return perrors.WithStack(errClientReadTimeout) case \u0026lt;-response.Done: err = response.Err } return perrors.WithStack(err) } 场景2：Context 的超时处理\nfunc main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() go doSomething() select { case \u0026lt;-ctx.Done(): fmt.Println(\u0026#34;main\u0026#34;, ctx.Err()) } } 2. 图解源码 #  2.1 四叉堆原理 #  timer 的全局堆是一个四叉堆，特别是 Go 1.14 之后每个 P 都会维护着一个四叉堆，减少了 Goroutine 之间的并发问题，提升了 timer 了性能。\n四叉堆其实就是四叉树，Go timer 是如何维护四叉堆的呢？\n Go runtime 调度 timer 时，触发时间更早的 timer，要减少其查询次数，尽快被触发。所以四叉树的父节点的触发时间是一定小于子节点的。 四叉树顾名思义最多有四个子节点，为了兼顾四叉树插、删除、重排速度，所以四个兄弟节点间并不要求其按触发早晚排序。  这里用两张动图简单演示下 timer 的插入和删除\n把 timer 插入堆 把 timer 从堆中删除 2.2 timer 是如何被调度的？ #   调用 NewTimer，timer.After, timer.AfterFunc 生产 timer, 加入对应的 P 的堆上。 调用 timer.Stop, timer.Reset 改变对应的 timer 的状态。 GMP 在调度周期内中会调用 checkTimers ，遍历该 P 的 timer 堆上的元素，根据对应 timer 的状态执行真的操作。   2.3 timer 是如何加入到 timer 堆上的？ #  把 timer 加入调度总共有下面几种方式：\n 通过 NewTimer, time.After, timer.AfterFunc 初始化 timer 后，相关 timer 就会被放入到对应 p 的 timer 堆上。 timer 已经被标记为 timerRemoved，调用了 timer.Reset(d)，这个 timer 也会重新被加入到 p 的 timer 堆上 timer 还没到需要被执行的时间，被调用了 timer.Reset(d)，这个 timer 会被 GMP 调度探测到，先将该 timer 从 timer 堆上删除，然后重新加入到 timer 堆上 STW 时，runtime 会释放不再使用的 p 的资源，p.destroy()-\u0026gt;timer.moveTimers，将不再被使用的 p 的 timers 上有效的 timer(状态是：timerWaiting，timerModifiedEarlier，timerModifiedLater) 都重新加入到一个新的 p 的 timer 上  2.4 Reset 时 timer 是如何被操作的？ #  Reset 的目的是把 timer 重新加入到 timer 堆中，重新等待被触发。不过分为两种情况：\n 被标记为 timerRemoved 的 timer，这种 timer 是已经从 timer 堆上删除了，但会重新设置被触发时间，加入到 timer 堆中 等待被触发的 timer，在 Reset 函数中只会修改其触发时间和状态（timerModifiedEarlier或timerModifiedLater）。这个被修改状态的 timer 也同样会被重新加入到 timer堆上，不过是由 GMP 触发的，由 checkTimers 调用 adjusttimers 或者 runtimer 来执行的。   2.5 Stop 时 timer 是如何被操作的？ #  time.Stop 为了让 timer 停止，不再被触发，也就是从 timer 堆上删除。不过 timer.Stop 并不会真正的从 p 的 timer 堆上删除 timer，只会将 timer 的状态修改为 timerDeleted。然后等待 GMP 触发的 adjusttimers 或者 runtimer 来执行。\n真正删除 timer 的函数有两个 dodeltimer，dodeltimer0。\n 2.6 Timer 是如何被真正执行的？ #  timer 的真正执行者是 GMP。GMP 会在每个调度周期内，通过 runtime.checkTimers 调用 timer.runtimer(). timer.runtimer 会检查该 p 的 timer 堆上的所有 timer，判断这些 timer 是否能被触发。\n如果该 timer 能够被触发，会通过回调函数 sendTime 给 Timer 的 channel C 发一个当前时间，告诉我们这个 timer 已经被触发了。\n如果是 ticker 的话，被触发后，会计算下一次要触发的时间，重新将 timer 加入 timer 堆中。\n 3. Timer 使用中的坑 #  确实 timer 是我们开发中比较常用的工具，但是 timer 也是最容易导致内存泄露，CPU 狂飙的杀手之一。\n不过仔细分析可以发现，其实能够造成问题就两个方面：\n 错误创建很多的 timer，导致资源浪费 由于 Stop 时不会主动关闭 C，导致程序阻塞  3.1 错误创建很多 timer，导致资源浪费 #  func main() { for { // xxx 一些操作  timeout := time.After(30 * time.Second) select { case \u0026lt;- someDone: // do something  case \u0026lt;-timeout: return } } } 上面这段代码是造成 timer 异常的最常见的写法，也是我们最容易忽略的写法。\n造成问题的原因其实也很简单，因为 timer.After 底层是调用的 timer.NewTimer，NewTimer 生成 timer 后，会将 timer 放入到全局的 timer 堆中。\nfor 会创建出来数以万计的 timer 放入到 timer 堆中，导致机器内存暴涨，同时不管 GMP 周期 checkTimers，还是插入新的 timer 都会疯狂遍历 timer 堆，导致 CPU 异常。\n要注意的是，不只 time.After 会生成 timer, NewTimer，time.AfterFunc 同样也会生成 timer 加入到 timer 中，也都要防止循环调用。\n解决办法: 使用 time.Reset 重置 timer，重复利用 timer。\n我们已经知道 time.Reset 会重新设置 timer 的触发时间，然后将 timer 重新加入到 timer 堆中，等待被触发调用。\nfunc main() { timer := time.NewTimer(time.Second * 5) for { t.Reset(time.Second * 5) select { case \u0026lt;- someDone: // do something  case \u0026lt;-timer.C: return } } } 3.2 程序阻塞，造成内存或者 goroutine 泄露 #  func main() { timer1 := time.NewTimer(2 * time.Second) \u0026lt;-timer1.C println(\u0026#34;done\u0026#34;) } 上面的代码可以看出来，只有等待 timer 超时 \u0026ldquo;done\u0026rdquo; 才会输出，原理很简单：程序阻塞在 \u0026lt;-timer1.C 上，一直等待 timer 被触发时，回调函数 time.sendTime 才会发送一个当前时间到 timer1.C 上，程序才能继续往下执行。\n不过使用 timer.Stop 的时候就要特别注意了，比如：\nfunc main() { timer1 := time.NewTimer(2 * time.Second) go func() { timer1.Stop() }() \u0026lt;-timer1.C println(\u0026#34;done\u0026#34;) } 程序就会一直死锁了，因为 timer1.Stop 并不会关闭 channel C，使程序一直阻塞在 timer1.C 上。\n上面这个例子过于简单了，试想下如果 \u0026lt;- timer1.C 是阻塞在子协程中，timer 被的 Stop 方法被调用，那么子协程可能就会被永远的阻塞在那里，造成 goroutine 泄露，内存泄露。\nStop 的正确的使用方式：\nfunc main() { timer1 := time.NewTimer(2 * time.Second) go func() { if !timer1.Stop() { \u0026lt;-timer1.C } }() select { case \u0026lt;-timer1.C: fmt.Println(\u0026#34;expired\u0026#34;) default: } println(\u0026#34;done\u0026#34;) } 到此，Go timer 基本已经结束了，有想跟我讨论的可以在留言区评论。\n Go timer 完整流程图获取链接：链接: 链接: https://pan.baidu.com/s/1nUvTK_0qBlwbS6LbZXKM7g 密码: t219 其他模块流程图，请关注公众号 HHFCodeRv 回复1获取。\n更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":3,"href":"/gohandbook.github.io/docs/rand-chapter/","title":"随机数","section":"Docs","content":"本章节是对 sync 包的剖析，包括：math/rand\n  math/rand  更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":4,"href":"/gohandbook.github.io/docs/sync-chapter/2021-05-10-sync-cond/","title":"Cond","section":"并发原语","content":"这一次，彻底搞懂 Go Cond #  hi，大家好，我是 haohongfan。\n本篇文章会从源码角度去深入剖析下 sync.Cond。Go 日常开发中 sync.Cond 可能是我们用的较少的控制并发的手段，因为大部分场景下都被 Channel 代替了。还有就是 sync.Cond 使用确实也蛮复杂的。\n比如下面这段代码：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { done := make(chan int, 1) go func() { time.Sleep(5 * time.Second) done \u0026lt;- 1 }() fmt.Println(\u0026#34;waiting\u0026#34;) \u0026lt;-done fmt.Println(\u0026#34;done\u0026#34;) } 同样可以使用 sync.Cond 来实现\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func main() { cond := sync.NewCond(\u0026amp;sync.Mutex{}) var flag bool go func() { time.Sleep(time.Second * 5) cond.L.Lock() flag = true cond.Signal() cond.L.Unlock() }() fmt.Println(\u0026#34;waiting\u0026#34;) cond.L.Lock() for !flag { cond.Wait() } cond.L.Unlock() fmt.Println(\u0026#34;done\u0026#34;) } 大部分场景下使用 channel 是比 sync.Cond方便的。不过我们要注意到，sync.Cond 提供了 Broadcast 方法，可以通知所有的等待者。想利用 channel 实现这个方法还是不容易的。我想这应该是 sync.Cond 唯一有用武之地的地方。\n先列出来一些问题吧，可以带着这些问题来阅读本文：\n cond.Wait本身就是阻塞状态，为什么 cond.Wait 需要在循环内 ？ sync.Cond 如何触发不能复制的 panic ? 为什么 sync.Cond 不能被复制 ？ cond.Signal 是如何通知一个等待的 goroutine ? cond.Broadcast 是如何通知等待的 goroutine 的？  源码剖析 #      cond.Wait 是阻塞的吗？是如何阻塞的？ #  是阻塞的。不过不是 sleep 这样阻塞的。\n调用 goparkunlock 解除当前 goroutine 的 m 的绑定关系，将当前 goroutine 状态机切换为等待状态。等待后续 goready 函数时候能够恢复现场。\ncond.Signal 是如何通知一个等待的 goroutine ? #   判断是否有没有被唤醒的 goroutine，如果都已经唤醒了，直接就返回了 将已通知 goroutine 的数量加1 从等待唤醒的 goroutine 队列中，获取 head 指针指向的 goroutine，将其重新加入调度 被阻塞的 goroutine 可以继续执行  cond.Broadcast 是如何通知等待的 goroutine 的？ #   判断是否有没有被唤醒的 goroutine，如果都已经唤醒了，直接就返回了 将等待通知的 goroutine 数量和已经通知过的 goroutine 数量设置成相等 遍历等待唤醒的 goroutine 队列，将所有的等待的 goroutine 都重新加入调度 所有被阻塞的 goroutine 可以继续执行  cond.Wait本身就是阻塞状态，为什么 cond.Wait 需要在循环内 ？ #  我们能注意到，调用 cond.Wait 的位置，使用的是 for 的方式来调用 wait 函数，而不是使用 if 语句。\n这是由于 wait 函数被唤醒时，存在虚假唤醒等情况，导致唤醒后发现，条件依旧不成立。因此需要使用 for 语句来循环地进行等待，直到条件成立为止。\n使用中注意点 #  1. 不能不加锁直接调用 cond.Wait #  func (c *Cond) Wait() { c.checker.check() t := runtime_notifyListAdd(\u0026amp;c.notify) c.L.Unlock() runtime_notifyListWait(\u0026amp;c.notify, t) c.L.Lock() } 我们看到 Wait 内部会先调用 c.L.Unlock()，来先释放锁。如果调用方不先加锁的话，会触发“fatal error: sync: unlock of unlocked mutex”。关于 mutex 的使用方法，推荐阅读下《这可能是最容易理解的 Go Mutex 源码剖析》\n2. 为什么不能 sync.Cond 不能复制 ？ #  sync.Cond 不能被复制的原因，并不是因为 sync.Cond 内部嵌套了 Locker。因为 NewCond 时传入的 Mutex/RWMutex 指针，对于 Mutex 指针复制是没有问题的。\n主要原因是 sync.Cond 内部是维护着一个 notifyList。如果这个队列被复制的话，那么就在并发场景下导致不同 goroutine 之间操作的 notifyList.wait、notifyList.notify 并不是同一个，这会导致出现有些 goroutine 会一直堵塞。\n这里留下一个问题，sync.Cond 内部是有一段代码 check sync.Cond 是不能被复制的，下面这段代码能触发这个 panic 吗？\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { cond1 := sync.NewCond(new(sync.Mutex)) cond := *cond1 fmt.Println(cond) } 有兴趣的可以动手尝试下，以及尝试下如何才能触发这个panic \u0026ldquo;sync.Cond is copied” 。\n更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":5,"href":"/gohandbook.github.io/docs/sync-chapter/","title":"并发原语","section":"Docs","content":"本章节是对 sync 包的剖析，包括：Mutex, Cond, sync.Map, WaitGroup, Pool\n  Mutex  Cond  sync.Map  WaitGroup  Pool  更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":6,"href":"/gohandbook.github.io/docs/sync-chapter/2021-05-10-sync-map/","title":"Sync.Map","section":"并发原语","content":"看过这篇剖析，你还不懂 Go sync.Map 吗 #  hi, 大家好，我是 haohongfan。\n本篇文章会从使用方式和原码角度剖析 sync.Map。不过不管是日常开发还是开源项目中，好像 sync.Map 并没有得到很好的利用，大家还是习惯使用 Mutex + Map 来使用。\n下面这段代码，看起来很有道理，其实是用错了（背景：并发场景中获取注册信息）。\ninstance, ok := instanceMap[name] if ok { return instance, nil } initLock.Lock() defer initLock.Unlock() // double check instance, ok = instanceMap[name] if ok { return instance, nil } 这里使用使用 sync.Map 会更合理些，因为 sync.Map 底层完全包含了这个逻辑。可能写 Java 的同学看着上面这段代码很眼熟，但确实是用错了，关于为什么用错了以及会造成什么影响，请大家关注后续的文章。\n我大概分析了下大家宁愿使用 Mutex + Map，也不愿使用 sync.Map 的原因：\n sync.Map 本身就很难用，使用起来并不像一个 Map。失去了 map 应有的特权语法，如：make, map[1] 等 sync.Map 方法较多。让一个简单的 Map 使用起来有了较高的学习成本。  不管什么样的原因吧，当你读过这篇文章后，在某些特定的并发场景下，建议使用 sync.Map 代替 Map + Mutex 的。\n用法全解 #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { var syncMap sync.Map syncMap.Store(\u0026#34;11\u0026#34;, 11) syncMap.Store(\u0026#34;22\u0026#34;, 22) fmt.Println(syncMap.Load(\u0026#34;11\u0026#34;)) // 11 \tfmt.Println(syncMap.Load(\u0026#34;33\u0026#34;)) // 空  fmt.Println(syncMap.LoadOrStore(\u0026#34;33\u0026#34;, 33)) // 33 \tfmt.Println(syncMap.Load(\u0026#34;33\u0026#34;)) // 33 \tfmt.Println(syncMap.LoadAndDelete(\u0026#34;33\u0026#34;)) // 33 \tfmt.Println(syncMap.Load(\u0026#34;33\u0026#34;)) // 空  syncMap.Range(func(key, value interface{}) bool { fmt.Printf(\u0026#34;key:%v value:%v\\n\u0026#34;, key, value) return true }) // key:22 value:22 \t// key:11 value:11 } 其实 sync.Map 并不复杂，只是将普通 map 的相关操作转成对应函数而已。\n    普通 map sync.Map     map 获取某个 key map[1] sync.Load(1)   map 添加元素 map[1] = 10 sync.Store(1, 10)   map 删除一个 key delete(map, 1) sync.Delete(1)   遍历 map for\u0026hellip;range sync.Range()    sync.Map 两个特有的函数，不过从字面就能理解是什么意思了。 LoadOrStore：sync.Map 存在就返回，不存在就插入 LoadAndDelete：sync.Map 获取某个 key，如果存在的话，同时删除这个 key\n源码解析 #  type Map struct { mu Mutex read atomic.Value // readOnly read map \tdirty map[interface{}]*entry // dirty map \tmisses int }     read map 的值是什么时间更新的 ？ #   Load/LoadOrStore/LoadAndDelete 时，当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map Store/LoadOrStore 时，当 read map 中存在这个key，则更新 Delete/LoadAndDelete 时，如果 read map 中存在这个key，则设置这个值为 nil  dirty map 的值是什么时间更新的 ？ #   完全是一个新 key， 第一次插入 sync.Map，必先插入 dirty map Store/LoadOrStore 时，当 read map 中不存在这个key，在 dirty map 存在这个key，则更新 Delete/LoadAndDelete 时，如果 read map 中不存在这个key，在 dirty map 存在这个key，则从 dirty map 中删除这个key 当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map，同时设置 dirty map 为 nil  疑问：当 dirty map 复制到 read map 后，将 dirty map 设置为 nil，也就是 dirty map 中就不存在这个 key 了。如果又新插入某个 key，多次访问后达到了 dirty map 往 read map 复制的条件，如果直接用 read map 覆盖 dirty map，那岂不是就丢了之前在 read map 但不在 dirty map 的 key ?\n答：其实并不会。当 dirty map 向 read map 复制后，readOnly.amended 等于了 false。当新插入了一个值时，会将 read map 中的值，重新给 dirty map 赋值一遍，也就是 read map 也会向 dirty map 中复制。\nfunc (m *Map) dirtyLocked() { if m.dirty != nil { return } read, _ := m.read.Load().(readOnly) m.dirty = make(map[interface{}]*entry, len(read.m)) for k, e := range read.m { if !e.tryExpungeLocked() { m.dirty[k] = e } } } read map 和 dirty map 是什么时间删除的？ #   当 read map 中存在某个 key 的时候，这个时候只会删除 read map， 并不会删除 dirty map（因为 dirty map 不存在这个值） 当 read map 中不存在时，才会去删除 dirty map 里面的值  疑问：如果按照这个删除方式，那岂不是 dirty map 中会有残余的 key，导致没删除掉？\n答：其实并不会。当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map。这个过程中还附带了另外一个操作：将 dirty map 置为 nil。\nfunc (m *Map) missLocked() { m.misses++ if m.misses \u0026lt; len(m.dirty) { return } m.read.Store(readOnly{m: m.dirty}) m.dirty = nil m.misses = 0 } read map 与 dirty map 的关系 ？ #   在 read map 中存在的值，在 dirty map 中可能不存在。 在 dirty map 中存在的值，在 read map 中也可能存在。 当访问多次，发现 dirty map 中存在，read map 中不存在，导致 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map。 当出现 dirty map 向 read map 复制后，dirty map 会被置成 nil。 当出现 dirty map 向 read map 复制后，readOnly.amended 等于了 false。当新插入了一个值时，会将 read map 中的值，重新给 dirty map 赋值一遍  read/dirty map 中的值一定是有效的吗？ #  并不一定。放入到 read/dirty map 中的值总共有 3 种类型：\n nil：如果获取到的 value 是 nil，那说明这个 key 是已经删除过的。既不在 read map，也不在 dirty map expunged：这个 key 在 dirty map 中是不存在的 valid：其实就正常的情况，要么这个值存在在 read map 中，要么存在在 dirty map 中  sync.Map 是如何提高性能的？ #  通过源码解析，我们知道 sync.Map 里面有两个普通 map，read map主要是负责读，dirty map 是负责读和写（加锁）。在读多写少的场景下，read map 的值基本不发生变化，可以让 read map 做到无锁操作，就减少了使用 Mutex + Map 必须的加锁/解锁环节，因此也就提高了性能。\n不过也能够看出来，read map 也是会发生变化的，如果某些 key 写操作特别频繁的话，sync.Map 基本也就退化成了 Mutex + Map（有可能性能还不如 Mutex + Map）。\n所以，不是说使用了 sync.Map 就一定能提高程序性能，我们日常使用中尽量注意拆分粒度来使用 sync.Map。\n关于如何分析 sync.Map 是否优化了程序性能，同样可以使用 pprof。具体过程可以参考 《这可能是最容易理解的 Go Mutex 源码剖析》\nsync.Map 应用场景 #   读多写少 写操作也多，但是修改的 key 和读取的 key 特别不重合。  关于第二点我觉得挺扯的，毕竟我们很难把控这一点，不过由于是官方的注释还是放在这里。\n实际开发中我们要注意使用场景和擅用 pprof 来分析程序性能。\nsync.Map 使用注意点 #  和 Mutex 一样， sync.Map 也同样不能被复制，因为 atomic.Value 是不能被复制的。\n更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":7,"href":"/gohandbook.github.io/docs/timer-chapter/","title":"定时器","section":"Docs","content":"本章节是对 timer 包的解析\n  timer  更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":8,"href":"/gohandbook.github.io/docs/sync-chapter/2021-05-10-sync-waitgroup/","title":"WaitGroup","section":"并发原语","content":"最清晰易懂的 Go WaitGroup 源码剖析 #  hi，大家好，我是haohongfan。\n本篇主要介绍 WaitGroup 的一些特性，让我们从本质上去了解 WaitGroup。关于 WaitGroup 的基本用法这里就不做过多介绍了。相对于《这可能是最容易理解的 Go Mutex 源码剖析》来说，WaitGroup 就简单的太多了。\n源码剖析 #    type WaitGroup struct { noCopy noCopy state1 [3]uint32 } WaitGroup 底层结构看起来简单，但 WaitGroup.state1 其实代表三个字段：counter，waiter，sema。\ncounter ：可以理解为一个计数器，计算经过 wg.Add(N), wg.Done() 后的值。 waiter ：当前等待 WaitGroup 任务结束的等待者数量。其实就是调用 wg.Wait() 的次数，所以通常这个值是 1 。 sema ： 信号量，用来唤醒 Wait() 函数。\n为什么要将 counter 和 waiter 放在一起 ？ #  其实是为了保证 WaitGroup 状态的完整性。举个例子，看下面的一段源码\n// sync/waitgroup.go:L79 --\u0026gt; Add() if v \u0026gt; 0 || w == 0 { // v =\u0026gt; counter, w =\u0026gt; waiter  return } // ... *statep = 0 for ; w != 0; w-- { runtime_Semrelease(semap, false, 0) } 当同时发现 wg.counter \u0026lt;= 0 \u0026amp;\u0026amp; wg.waiter != 0 时，才会去唤醒等待的 waiters，让等待的协程继续运行。但是使用 WaitGroup 的调用方一般都是并发操作，如果不同时获取的 counter 和 waiter 的话，就会造成获取到的 counter 和 waiter 可能不匹配，造成程序 deadlock 或者程序提前结束等待。\n如何获取 counter 和 waiter ? #  对于 wg.state 的状态变更，WaitGroup 的 Add()，Wait() 是使用 atomic 来做原子计算的(为了避免锁竞争)。但是由于 atomic 需要使用者保证其 64 位对齐，所以将 counter 和 waiter 都设置成 uint32，同时作为一个变量，即满足了 atomic 的要求，同时也保证了获取 waiter 和 counter 的状态完整性。但这也就导致了 32位，64位机器上获取 state 的方式并不相同。如下图： 简单解释下：\n因为 64 位机器上本身就能保证 64 位对齐，所以按照 64 位对齐来取数据，拿到 state1[0], state1[1] 本身就是64 位对齐的。但是 32 位机器上并不能保证 64 位对齐，因为 32 位机器是 4 字节对齐，如果也按照 64 位机器取 state[0]，state[1] 就有可能会造成 atmoic 的使用错误。\n于是 32 位机器上空出第一个 32 位，也就使后面 64 位天然满足 64 位对齐，第一个 32 位放入 sema 刚好合适。早期 WaitGroup 的实现 sema 是和 state1 分开的，也就造成了使用 WaitGroup 就会造成 4 个字节浪费，不过 go1.11 之后就是现在的结构了。\n为什么流程图里缺少了 Done ? #  其实并不是，是因为 Done 的实现就是 Add. 只不过我们常规用法 wg.Add(1) 是加 1 ，wg.Done() 是减 1，即 wg.Done() 可以用 wg.Add(-1) 来代替。 尽管我们知道 wg.Add 可以传递负数当 wg.Done 使用，但是还是别这么用。\n退出waitgroup的条件 #  其实就一个条件， WaitGroup.counter 等于 0\n日常开发中特殊需求 #  1. 控制超时/错误控制 #  虽说 WaitGroup 能够让主 Goroutine 等待子 Goroutine 退出，但是 WaitGroup 遇到一些特殊的需求，如：超时，错误控制，并不能很好的满足，需要做一些特殊的处理。 ** 真实场景：\n用户在电商平台中购买某个货物，为了计算用户能优惠的金额，需要去获取 A 系统（权益系统），B 系统（角色系统），C 系统（商品系统），D 系统（xx系统）。为了提高程序性能，可能会同时发起多个 Goroutine 去访问这些系统，必然会使用 WaitGroup 等待数据的返回，但是存在一些问题：\n 当某个系统发生错误，等待的 Goroutine 如何感知这些错误？ 当某个系统响应过慢，等待的 Goroutine 如何控制访问超时？  这些问题都是直接使用 WaitGroup 没法处理的。如果直接使用 channel 配合 WaitGroup 来控制超时和错误返回的话，封装起来并不简单，而且还容易出错。我们可以采用 ErrGroup 来代替 WaitGroup。\n有关 ErrGroup 的用法这里就不再阐述。golang.org/x/sync/errgroup\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/sync/errgroup\u0026#34; \u0026#34;time\u0026#34; ) func main() { ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() errGroup, newCtx := errgroup.WithContext(ctx) done := make(chan struct{}) go func() { for i := 0; i \u0026lt; 10; i++ { errGroup.Go(func() error { time.Sleep(time.Second * 10) return nil }) } if err := errGroup.Wait(); err != nil { fmt.Printf(\u0026#34;do err:%v\\n\u0026#34;, err) return } done \u0026lt;- struct{}{} }() select { case \u0026lt;-newCtx.Done(): fmt.Printf(\u0026#34;err:%v \u0026#34;, newCtx.Err()) return case \u0026lt;-done: } fmt.Println(\u0026#34;success\u0026#34;) } 2. 控制 Goroutine 数量 #  场景模拟： 大概有 2000 - 3000 万个数据需要处理，根据对服务器的测试，当启动 200 个 Goroutine 处理时性能最佳。如何控制？\n遇到诸如此类的问题时，单纯使用 WaitGroup 是不行的。既要保证所有的数据都能被处理，同时也要保证同时最多只有 200 个 Goroutine。这种问题需要 WaitGroup 配合 Channel 一块使用。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func main() { var wg = sync.WaitGroup{} manyDataList := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} ch := make(chan bool, 3) for _, v := range manyDataList { wg.Add(1) go func(data int) { defer wg.Done() ch \u0026lt;- true fmt.Printf(\u0026#34;go func: %d, time: %d\\n\u0026#34;, data, time.Now().Unix()) time.Sleep(time.Second) \u0026lt;-ch }(v) } wg.Wait() } 使用注意点 #  使用 WaitGroup 同样不能被复制。具体例子就不再分析了。具体分析过程可以参见《这可能是最容易理解的 Go Mutex 源码剖析》\nWaitGroup 的剖析到这里基本就结束了。有什么想跟我交流的，欢迎评论区留言。\n更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "},{"id":9,"href":"/gohandbook.github.io/docs/sync-chapter/2021-05-22-sync-pool/","title":"sync.Pool","section":"并发原语","content":"Go sync.Pool 浅析 #  hi, 大家好，我是 haohongfan。\nsync.Pool 应该是 Go 里面明星级别的数据结构，有很多优秀的文章都在介绍这个结构，本篇文章简单剖析下 sync.Pool。不过说实话 sync.Pool 并不是我们日常开发中使用频率很高的的并发原语。\n尽管用的频率很低，但是不可否认的是 sync.Pool 确实是 Go 的杀手锏，合理使用 sync.Pool 会让我们的程序性能飙升。本篇文章会从使用方式，源码剖析，运用场景等方面，让你对 sync.Pool 有一个清晰的认知。\n使用方式 #  sync.Pool 使用很简单，但是想用对却很麻烦，因为你有可能看到网上一堆错误的示例，各位同学在搜索 sync.Pool 的使用例子时，要特别注意。\nsync.Pool 是一个内存池。通常内存池是用来防止内存泄露的（例如C/C++)。sync.Pool 这个内存池却不是干这个的，带 GC 功能的语言都存在垃圾回收 STW 问题，需要回收的内存块越多，STW 持续时间就越长。如果能让 new 出来的变量，一直不被回收，得到重复利用，是不是就减轻了 GC 的压力。\n正确的使用示例（下面的demo选自gin）\nfunc (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) engine.pool.Put(c) } 一定要注意的是：是先 Get 获取内存空间，基于这个内存做相关的处理，然后再将这个内存还回（Put）到 sync.Pool。\nPool 结构 #   源码图解 #    简单点可以总结成下面的流程：\n   Sync.Pool 梳理 #  Pool 的内容会清理？清理会造成数据丢失吗？ #  Go 会在每个 GC 周期内定期清理 sync.Pool 内的数据。\n要分几个方面来说这个问题。\n 已经从 sync.Pool Get 的值，在 poolClean 时虽说将 pool.local 置成了nil，Get 到的值依然是有效的，是被 GC 标记为黑色的，不会被 GC回收，当 Put 后又重新加入到 sync.Pool 中 在第一个 GC 周期内 Put 到 sync.Pool 的数值，在第二个 GC 周期没有被 Get 使用，就会被放在 local.victim 中。如果在 第三个 GC 周期仍然没有被使用就会被 GC 回收。  runtime.GOMAXPROCS 与 pool 之间的关系？ #  s := p.localSize l := p.local if uintptr(pid) \u0026lt; s { return indexLocal(l, pid), pid } if p.local == nil { allPools = append(allPools, p) } // If GOMAXPROCS changes between GCs, we re-allocate the array and lose the old one. size := runtime.GOMAXPROCS(0) local := make([]poolLocal, size) atomic.StorePointer(\u0026amp;p.local, unsafe.Pointer(\u0026amp;local[0])) // store-release runtime_StoreReluintptr(\u0026amp;p.localSize, uintptr(size)) // store-release runtime.GOMAXPROCS(0) 是获取当前最大的 p 的数量。sync.Pool 的 poolLocal 数量受 p 的数量影响，会开辟 runtime.GOMAXPROCS(0) 个 poolLocal。某些场景下我们会使用 runtime.GOMAXPROCS（N) 来改变 p 的数量，会使 sync.Pool 的 pool.poolLocal 释放重新开辟新的空间。\n为什么要开辟 runtime.GOMAXPROCS 个 local？\npool.local 是个 poolLocal 结构，这个结构体是 private + shared链表组成，在多 goroutine 的 Get/Put 下是有数据竞争的，如果只有一个 local 就需要加锁来操作。每个 p 的 local 就能减少加锁造成的数据竞争问题。\nNew() 的作用？假如没有 New 会出现什么情况？ #  从上面的 pool.Get 流程图可以看出来，从 sync.Pool 获取一个内存会尝试从当前 private，shared，其他的 p 的 shared 获取或者 victim 获取，如果实在获取不到时，才会调用 New 函数来获取。也就是 New() 函数才是真正开辟内存空间的。New() 开辟出来的的内存空间使用完毕后，调用 pool.Put 函数放入到 sync.Pool 中被重复利用。\n如果 New 函数没有被初始化会怎样呢？很明显，sync.Pool 就废掉了，因为没有了初始化内存的地方了。\n先 Put，再 Get 会出现什么情况？ #  一定要注意，下面这个例子的用法是错误的\nfunc main(){ pool:= sync.Pool{ New: func() interface{} { return item{} }, } pool.Put(item{value:1}) data := pool.Get() fmt.Println(data) } 如果你直接跑这个例子，能得到你想像的结果，但是在某些情况下就不是这个结果了。\n在 Pool.Get 注释里面有这么一句话：“Callers should not assume any relation between values passed to Put and the values returned by Get.”，告诉我们不能把值 Pool.Put 到 sync.Pool 中，再使用 Pool.Get 取出来，因为 sync.Pool 不是 map 或者 slice，放入的值是有可能拿不到的，sync.Pool 的数据结构就不支持做这个事情。\n前面说使用 sync.Pool 容易被错误示例误导，就是上面这个写法。为什么 Put 的值 再 Get 会出现问题？\n 情况1：sync.Pool 的 poolCleanup 函数在系统 GC 时会被调用，Put 到 sync.Pool 的值，由于有可能一直得不到利用，被在某个 GC 周期内就有可能被释放掉了。 情况2：不同的 goroutine 绑定的 p 有可能是不一样的，当前 p 对应的 goroutine 放入到 sync.Pool 的值有可能被其他的 p 对应的 goroutine 取到，导致当前 goroutine 再也取不到这个值。 情况3：使用 runtime.GOMAXPROCS（N) 来改变 p 的数量，会使 sync.Pool 的 pool.poolLocal 释放重新开辟新的空间，导致 sync.Pool 被释放掉。 情况4：还有很多情况  只 Get 不 Put 会内存泄露吗？ #  使用其他的池，如连接池，如果取连接使用后不放回连接池，就会出现连接池泄露，是不是 sync.Pool 也有这个问题呢？\n通过上面的流程图，可以看出来 Pool.Get 的时候会尝试从当前 private，shared，其他的 p 的 shared 获取或者 victim 获取，如果实在获取不到时，才会调用 New 函数来获取，New 出来的内容本身还是受系统 GC 来控制的。所以如果我们提供的 New 实现不存在内存泄露的话，那么 sync.Pool 是不会内存泄露的。当 New 出来的变量如果不再被使用，就会被系统 GC 给回收掉。\n如果不 Put 回 sync.Pool，会造成 Get 的时候每次都调用的 New 来从堆栈申请空间，达不到减轻 GC 压力。\n使用场景 #  上面说到 sync.Pool 业务开发中不是一个常用结构，我们业务开发中没必要假想某块代码会有强烈的性能问题，一上来就用 sync.Pool 硬怼。 sync.Pool 主要是为了解决 Go GC 压力过大问题的，所以一般情况下，当线上高并发业务出现 GC 问题需要被优化时，才需要用 sync.Pool 出场。\n使用注意点 #   sync.Pool 同样不能被复制。 好的使用习惯，从 pool.Get 出来的值进行数据的清空（reset），防止垃圾数据污染。   本文基于的 Go 源码版本：1.16.2\n 参考链接 #   深度解密 Go 语言之 sync.Pool https://www.cnblogs.com/qcrao-2018/p/12736031.html 请问sync.Pool有什么缺点？ https://mp.weixin.qq.com/s/2ZC1BWTylIZMmuQ3HwrnUg Go 1.13中 sync.Pool 是如何优化的? https://colobu.com/2019/10/08/how-is-sync-Pool-improved-in-Go-1-13/  sync.Pool 的剖析到这里基本就写完了，想跟我交流的可以在评论区留言。\n更多学习学习资料分享，关注公众号回复指令：\n 回复 0，获取 《Go 面经》 回复 1，获取 《Go 源码流程图》   "}]